{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "import dask.dataframe as dd\n",
    "import dask.bag as db\n",
    "import dask.diagnostics as dg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## figure out which stations we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {column name:extents of the fixed-width fields}\n",
    "columns = {\"ID\": (0,11), \"LATITUDE\": (12, 20), \"LONGITUDE\": (21, 30), \"ELEVATION\": (31, 37),\"STATE\": (38, 40),\n",
    "           \"NAME\": (41, 71), \"GSN FLAG\": (72, 75), \"HCN/CRN FLAG\": (76, 79),\"WMO ID\": (80, 85)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_fwf(\"http://noaa-ghcn-pds.s3.amazonaws.com/ghcnd-stations.txt\", \n",
    "                    colspecs=list(columns.values()), names=list(columns.keys()),\n",
    "                    dtype={'ID': str, 'LATITUDE':float, 'LONGITUDE':float, \n",
    "                           'ELEVATION':float, 'STATE':str, 'NAME':str, \n",
    "                           'GSN FLAG': str, 'HCN/CRN FLAG': str, 'WMO ID':str})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our documentation sheet https://docs.opendata.aws/noaa-ghcn-pds/readme.html\n",
    "ID = the station identification code.\n",
    "* The first two characters denote the FIPS country code\n",
    "So we're going to use that to get all the US Station IDs, but first we're going to check what it returns, which \n",
    "is states + DC + PR + VI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['ID'].str.startswith('US')]['STATE'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So lets get all the US country codes where the STATE is not in ['PR', 'VI'] 'cause it's easier to map\n",
    "# Also we need a WMO ID 'cause thats how we cross reference against mos\n",
    "# this is just a check that our filter is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (df['ID'].str.startswith('US') & ~ df['STATE'].isin(['PR', 'VI']) & ~df['WMO ID'].isnull())\n",
    "df[mask]['STATE'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we're getting the list of US unique stations\n",
    "us_stations = df[mask]\n",
    "len(us_stations['ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_stations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets sample that!\n",
    "Do we really need all our stations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.GeoDataFrame(us_stations, geometry=gpd.points_from_xy(us_stations['LONGITUDE'], us_stations['LATITUDE']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.plot(figsize=(10,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's find those outliers by looking at the longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.sort_values('LONGITUDE', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's drop them:\n",
    "gdf_w = gdf[gdf['LONGITUDE']<0]\n",
    "gdf_w.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We really don't need that dense of a network of plots, let's sample by 10 percent and see what happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_w.sample(frac=.10).plot(figsize=(10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1% which is about 6 stations\n",
    "gdf_w.sample(frac=.01).plot(figsize=(10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try your own \n",
    "frac = ?\n",
    "gdf_w.sample(frac=frac).plot(figsize=(10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets use the 60 stations from our sample to filter down our dataset\n",
    "us_stations = gdf_w.sample(frac=.1)['ID']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter MOS down to our sample set of stations\n",
    "OUR MOS codes aren't matching up w/ our WMO above so we're gonna find the nearest neighbors to the above stations\n",
    "This is a spreadsheet of where the mos stations are "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf = pd.read_csv(\"https://www.weather.gov/source/mdl/tables/MOS_stationtable_20190702.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# us stations only have 4 values\n",
    "mosdf = mdf[mdf['Station'].str.len()==4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mgdf = gpd.GeoDataFrame(mosdf,geometry=gpd.points_from_xy(mosdf['Longitude'], mosdf['Latitude']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mgdf['Station'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's find our nearest neighbor\n",
    "https://gis.stackexchange.com/questions/222315/geopandas-find-nearest-point-in-other-dataframe/222316"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute distance between each station and all the others\n",
    "def distance(row):\n",
    "    return mgdf['geometry'].distance(row['geometry'])\n",
    "    \n",
    "distances = gdf_w.apply(distance, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances.shape # rows in gdf_w x rows in mgdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the id of the closet station by finding the index of the station with the minimum distance\n",
    "# and then create a new column in our gdf_w with the closest MOS station\n",
    "stationloc = np.argmin(distances.values,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stationloc.shape, mgdf.shape, gdf_w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mgdf pull the station closest in distance to the gdf station for that row\n",
    "gdf_w['neighbor'] = mgdf.iloc[stationloc]['Station'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is our lookup comparison\n",
    "gdf_w[['ID', 'neighbor']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets merge in the mgdf since we're going to end up wanting the center lat/lon for plotting\n",
    "gdmo = gdf_w.merge(mgdf, left_on = 'neighbor', right_on='Station', suffixes=('_ghcn', '_mos'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdmo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's save out what we need:\n",
    "gdmo[['ID', 'LATITUDE', 'LONGITUDE', 'Station', 'Latitude', 'Longitude']].to_csv(\"ghcn_mos_lookup.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter GHCN down to our sample set of stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the above lines up w/ mos, let's filter GHCN down to the us_stations\n",
    "YEAR = 2017\n",
    "names = ['ID', 'DATE', 'ELEMENT', 'DATA_VALUE', 'M-FLAG', 'Q-FLAG', 'S-FLAG', 'OBS-TIME']\n",
    "ds = dd.read_csv(f's3://noaa-ghcn-pds/csv/{YEAR}.csv', storage_options={'anon':True},  \n",
    "                 names=names, dtype={'DATA_VALUE':'object'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsus = ds[ds['ID'].isin(gdmo['ID'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets only get the columns we want\n",
    "dsus_subset = dsus[['ID', 'DATE', 'ELEMENT', 'DATA_VALUE',  'M-FLAG', 'Q-FLAG']]\n",
    "dsus_subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's save out to a single largeish CSV\n",
    "sample = 100\n",
    "dsus.compute().to_csv(f\"GHCN_{YEAR}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter MOS down to just stations in our lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mos = dd.read_csv(f'Examples/mos/modelrun/mav{YEAR}*.csv', \n",
    "                  dtype={'CLD':object, 'OBV':object, 'TYP':object,\n",
    "                         'Q06':object, 'Q12':object, 'T06':object, \n",
    "                         'T12':object}).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mos[mos['station'].isin(gdmo['Station'])].to_csv(f\"MOS_{YEAR}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cuny-summer-2020]",
   "language": "python",
   "name": "conda-env-cuny-summer-2020-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
